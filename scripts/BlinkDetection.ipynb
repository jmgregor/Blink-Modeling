{
 "cells": [
  {
   "source": [
    "# Imaging Science Senior Project\n",
    "### Jared Gregor (jmg2586@rit.edu)\n",
    "The purpose of this Python Notebook is to provide a pipeline to allow for Blink modeling from eye tracking video. \n",
    "\n",
    "As video based eye trackers are becoming more common, eye tracking algorithms are becoming more complex to improve accuracy. Many of these algorithms utilize the pupil location to calculate other things such as gaze location. When a person makes a blink, the pupil is temporarily occluded, causing the algorithms to fail. Future algorithms may try to leverage the frames before a blink to accurately determine where the pupil will be after the blink. To do this, the blink needs to be quantified and modeled.\n",
    "\n",
    "All of the helper functions that were written by Jared are found in BlinkDetection_Functions.py"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import BlinkDetection_Functions"
   ]
  },
  {
   "source": [
    "## Evaluate Dataset\n",
    "It is useful to understand what the dataset you're working with is. For this project, I will be using the Gaze in Wild dataset. The function below makes a csv with the number of videos per trial for every person and every eye. The output csv is saved in 'data/outputs'. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of videos per person per eye for all experiments in dataset\n",
    "dataset = '../data/giw-blink/'\n",
    "BlinkDetection_Functions.datasetStats(dataset) # Creates csv with number of videos in dataset"
   ]
  },
  {
   "source": [
    "In the raw dataset, all of the left eyes are rotated 180 degrees. \n",
    "\n",
    "To stay consistant while labeling, it is important that all of the labeled images are oriented in the same direction. Let's roatate all of the videos 180 degrees to fix this issue. \n",
    "\n",
    "It may be a bit more efficiant if instead of rotating all of the frames in each video, we only rotate the frames that will be used for labeling. If DeepLabCut was already used to extract frames, the second function below will rotate only those images. \n",
    "\n",
    "Note: Only use one or the other!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate the dataset videos 180 degrees\n",
    "# Not Recommended\n",
    "path_video = \"../data/giw-blink/left/Person_1_Trial_1\"\n",
    "path_video = \"J:/jmgre/Documents/Senior Project/data/giw-blink-unsorted/left\"\n",
    "BlinkDetection_Functions.rotateVideos(path_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate the DLC extracted frames 180 degrees\n",
    "path_frames = \"../DeepLabCut/DLC_10_01_20/10-01-20-P4T1L-2020-10-01/labeled-data\"\n",
    "BlinkDetection_Functions.rotateFrames(path_frames)"
   ]
  },
  {
   "source": [
    "We can also reorganize the data to combine all of the data into one place. This will give us a good variety of images to be labeled and will hopefully work well across people."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'J:/jmgre/Documents/Senior Project/data/giw-blink-unsorted'\n",
    "BlinkDetection_Functions.mixData(path)"
   ]
  },
  {
   "source": [
    "## Setup a DeepLabCut Project\n",
    "\n",
    "https://github.com/DeepLabCut/DeepLabCut\n",
    "\n",
    "DeepLabCut (DLC) is a toolbox for markerless pose estimation. In this project, we will be utilizing DLC to train a model that is capable of tracking the upper and lower eyelids in the GIW dataset. DLC has well documented pipelines on their website for how to train such a model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplabcut"
   ]
  },
  {
   "source": [
    "DLC can be ran entirely though a GUI, or each step can be ran as a Python function. In this project, both options are available.\n",
    "\n",
    "### Option 1: Run DLC entirely through the GUI"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.launch_dlc()"
   ]
  },
  {
   "source": [
    "### Option 2: Create new DLC project"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'blink' # Enter the name of your experiment Task\n",
    "experimenter = 'p1e0t1' # Enter the name of the experimenter\n",
    "video = [\"/../data/giw-blink/right/Person_1_Trial_1/\"] # Enter the path of the folder to grab frames from\n",
    "\n",
    "path_config_file = deeplabcut.create_new_project(task,experimenter,video,copy_videos=True,working_directory='../DeepLabCut') "
   ]
  },
  {
   "source": [
    "### Option 3: Open an existing DLC project"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_config_file = \"/../DeepLabCut/blink-p1e0t1-2020-11-19/config.yaml\" # Enter the path to the project YAML file\n",
    "path_config_file = \"J:/jmgre/Documents/Senior Project/DeepLabCut/BlinkModeling-GIW-2020-11-20/config.yaml\""
   ]
  },
  {
   "source": [
    "## Edit the config.yaml file\n",
    "Now with the project created, the config.yaml file needs to be edited. In this file define the number of frames to be extracted, body part labels, etc. The body part labels are the points that get selected for labeling. In this project, body part labels will be all the points on the upper and lower eyelids to be selected. \n",
    "\n",
    "More information about our specific labeling scheme can be found here: \n",
    "\n",
    "https://docs.google.com/document/d/1dg3mrVQUD0TRQfZFkRGLiroeJcudWhvhZFnq21o9jFs/edit?usp=sharing\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## Extract Frames from videos\n",
    "Frames need to be extracted from the dataset videos to be labeled. DLC's default frame selection is to use kmeans. \n",
    "\n",
    "DeepLabCut offers several options as to how these frames will be selected. To see these options, add a '?' after the function.\n",
    "\n",
    "deeplabcut.extract_frames?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "deeplabcut.extract_frames(path_config_file)"
   ]
  },
  {
   "source": [
    "## Label frames\n",
    "Once successfully extracted, each of the frames needs to be labeled. For this project, I will be labeling the upper eyelid, lower eyelid, and pupil. The labeling scheme is defined in the config.yaml file. Both eyelids will have 15 markers labeled left to right. The markers should line up along the edge of the eyeball and the eyelid. The pupil will be labeled with markers in each of the cardinal directions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gui wx\n",
    "\n",
    "deeplabcut.label_frames(path_config_file)"
   ]
  },
  {
   "source": [
    "### Check the labels\n",
    "It is worth checking the labels before training to be sure everything goes properly. The DLC check_labels function checks to be sure the labels were created and stored properly."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.check_labels(path_config_file) #this creates a subdirectory with the frames + your labels"
   ]
  },
  {
   "source": [
    "### Create Training dataset\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.create_training_dataset(path_config_file)"
   ]
  },
  {
   "source": [
    "## Start training:\n",
    "deeplabcut.train_network?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.train_network(path_config_file)"
   ]
  },
  {
   "source": [
    "## Evaluate network:\n",
    "Now that we have trained our network, it is time to evaluate it's performance. DLC has a built in function for evaluating the network. \n",
    "\n",
    "deeplabcut.evaluate_network?\n",
    "\n",
    "There are also a few optional steps we can take here including extracting outliers, refining the labels, and retraining the network. These steps should be referenced by the DLC documentation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.evaluate_network(path_config_file, plotting=True)"
   ]
  },
  {
   "source": [
    "## Analyze new video\n",
    "Now that we have a model that we are happy with, we can use it to analyze new eye tracking videos.\n",
    "\n",
    "The results are stored in the same path as the input video."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videofile_path = ['videos/video3.avi','videos/video4.avi'] #Enter a folder OR a list of videos to analyze.\n",
    "\n",
    "deeplabcut.analyze_videos(path_config_file,videofile_path, videotype='.avi')"
   ]
  },
  {
   "source": [
    "## Create labeled video (optional)\n",
    "To illustrate the results of our model on the new video, DLC provides a function to produce videos with the markers overlaid. There are a ton of options they give us for creating these videos!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.create_labeled_video?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.create_labeled_video(path_config_file,videofile_path)"
   ]
  },
  {
   "source": [
    "## Fit polynomial to eyelids\n",
    "DeepLabCut will output a spreadsheet with each bodypart found and it's x and y coordinates. Using these points, we can fit a polynomial to each of the eyelids in each frame."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "source": [
    "## Calculate Blink Statistics\n",
    "\n",
    "\n",
    "\n",
    "### Percent closure / Blink amplitude\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Blink frequency (requires time)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### Blink Duration (requires time)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Output for RIT-eyes\n",
    "What will we actually be giving to RIT eyes so that the eyelids can be moving in blender similar to how we describe the shape in the video?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('DLC-GPU': conda)",
   "metadata": {
    "interpreter": {
     "hash": "e9fdffd969e677f5d620684da454766543548930fb13292fe1be38cf15475f6d"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}